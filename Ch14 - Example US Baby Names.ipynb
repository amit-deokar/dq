{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Textbook Chapter 14: Data Analysis Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**NOTE**: Use the following cell as standard import. You may need to ocassionally import additional Python packages, but this should work for most parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70y74dpQeikF"
   },
   "outputs": [],
   "source": [
    "!pip install -U pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "\n",
    "import os\n",
    "\n",
    "# scipy imports\n",
    "# There are several universal functions for numpy arrays that are available through the scipy package\n",
    "import scipy as sc\n",
    "from scipy import stats, integrate\n",
    "from scipy.stats.mstats import mode\n",
    "\n",
    "# numpy imports\n",
    "# pandas depends on numpy\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, threshold=500, suppress=True)\n",
    "np.random.seed(12345)\n",
    "np.random.seed(sum(map(ord, \"distributions\")))\n",
    "\n",
    "# pandas imports\n",
    "# The convention is to import pandas package with a pd prefix. \n",
    "# Also, since we most commonly use Series and DataFrame classes from this package, \n",
    "# we import them into the current namespace, so we do not have to refer to them with the pd prefix.\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "pd.set_option('display.max_columns', None) # enables showing all columns\n",
    "pd.options.display.max_rows = 20\n",
    "PREVIOUS_MAX_ROWS = pd.options.display.max_rows\n",
    "pd.options.display.notebook_repr_html = True\n",
    "\n",
    "# matplotlib imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "plt.subplots(figsize=(10,6))\n",
    "%matplotlib inline\n",
    "\n",
    "# seaborn imports\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# bokeh imports\n",
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#warnings.filterwarnings(action='once') #enable if needed to see the warning the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see the following warning:\n",
    "```\n",
    "FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
    "  import pandas.util.testing as tm\n",
    "```\n",
    "That warning has something to do with the `Seaborn` libary needing to adjust their code to be compatible to the latest version of `Pandas` API. It will likely be fixed in near future. You can ignore that warning right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AWp9PCOCeij7"
   },
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example 3. US Baby Names 1880–2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The United States Social Security Administration (SSA) has made available data on the frequency of baby names from 1880 through the present. Hadley Wickham, an author of several popular R packages, has often made use of this dataset in illustrating data manipulation in R.\n",
    "\n",
    "We need to do some data wrangling to load this dataset, but once we do that we will have a DataFrame that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "In [4]: names.head(10)\n",
    "Out[4]:\n",
    "        name sex  births  year\n",
    "0       Mary   F    7065  1880\n",
    "1       Anna   F    2604  1880\n",
    "2       Emma   F    2003  1880\n",
    "3  Elizabeth   F    1939  1880\n",
    "4     Minnie   F    1746  1880\n",
    "5   Margaret   F    1578  1880\n",
    "6        Ida   F    1472  1880\n",
    "7      Alice   F    1414  1880\n",
    "8     Bertha   F    1320  1880\n",
    "9      Sarah   F    1288  1880\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things you might want to do with the dataset:\n",
    "- Visualize the proportion of babies given a particular name (your own, or another name) over time\n",
    "- Determine the relative rank of a name\n",
    "- Determine the most popular names in each year or the names whose popularity has advanced or declined the most\n",
    "- Analyze trends in names: vowels, consonants, length, overall diversity, changes in spelling, first and last letters\n",
    "- Analyze external sources of trends: biblical names, celebrities, demographic changes\n",
    "\n",
    "With the tools in this book, many of these kinds of analyses are within reach, so I will walk you through some of them.\n",
    "As of this writing, the US Social Security Administration makes available data files, one per year, containing the total number of births for each sex/name combination. The raw archive of these files can be obtained from http://www.ssa.gov/oact/babynames/limits.html.\n",
    "\n",
    "In the event that this page has been moved by the time you’re reading this, it can most likely be located again by an internet search. After downloading the \"National data\" file `names.zip` and unzipping it, you will have a directory containing a series of files like `yob1880.txt`. \n",
    "\n",
    "We can use the Unix head command to look at the first 10 lines of one of the files (on Windows, you can use the more command or open it in a text editor). Alternatively, we can use magic command from within the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!head -n 10 ../data/babynames/yob1880.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is already in a nicely comma-separated form, it can be loaded into a DataFrame with `pandas.read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "currdir = %pwd\n",
    "datadir = os.path.join(currdir, '../data/babynames/')\n",
    "datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "names1880 = pd.read_csv(datadir + 'yob1880.txt', names=['name', 'sex', 'births'])\n",
    "names1880"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files only contain names with at least five occurrences in each year, so for simplicity’s sake we can use the sum of the births column by sex as the total number of births in that year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "names1880.groupby('sex').births.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is split into files by year, one of the first things to do is to assemble all of the data into a single DataFrame and further to add a year field. You can do this using `pandas.concat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "years = range(1880, 2011)\n",
    "\n",
    "pieces = []\n",
    "columns = ['name', 'sex', 'births']\n",
    "\n",
    "for year in years:\n",
    "    path = datadir + 'yob%d.txt' % year\n",
    "    frame = pd.read_csv(path, names=columns)\n",
    "\n",
    "    frame['year'] = year\n",
    "    pieces.append(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple things to note here. First, remember that concat glues the DataFrame objects together row-wise by default. Secondly, you have to pass `ignore_index=True` because we’re not interested in preserving the original row numbers returned from `read_csv`. So we now have a very large DataFrame containing all of the names data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate everything into a single DataFrame\n",
    "names = pd.concat(pieces, ignore_index=True)\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask for parallel computing in Python\n",
    "\n",
    "[Dask](https://docs.dask.org/) is a flexible library for parallel computing in Python.\n",
    "\n",
    "Dask is composed of two parts:\n",
    "\n",
    "- Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.\n",
    "- “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.\n",
    "\n",
    "Dask emphasizes the following virtues:\n",
    "\n",
    "- Familiar: Provides parallelized NumPy array and Pandas DataFrame objects\n",
    "- Flexible: Provides a task scheduling interface for more custom workloads and integration with other projects.\n",
    "- Native: Enables distributed computing in pure Python with access to the PyData stack.\n",
    "- Fast: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\n",
    "- Scales up: Runs resiliently on clusters with 1000s of cores\n",
    "- Scales down: Trivial to set up and run on a laptop in a single process\n",
    "- Responsive: Designed with interactive computing in mind, it provides rapid feedback and diagnostics to aid humans\n",
    "\n",
    "#### [Common Uses and Anti-Uses](https://docs.dask.org/en/latest/dataframe.html)\n",
    "\n",
    "Dask DataFrame is used in situations where Pandas is commonly needed, usually when Pandas fails due to data size or computation speed:\n",
    "\n",
    "- Manipulating large datasets, even when those datasets don’t fit in memory\n",
    "- Accelerating long computations by using many cores\n",
    "- Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations\n",
    "\n",
    "Dask DataFrame may not be the best choice in the following situations:\n",
    "\n",
    "- If your dataset fits comfortably into RAM on your laptop, then you may be better off just using Pandas. There may be simpler ways to improve performance than through parallelism\n",
    "- If your dataset doesn’t fit neatly into the Pandas tabular model, then you might find more use in dask.bag or dask.array\n",
    "- If you need functions that are not implemented in Dask DataFrame, then you might want to look at dask.delayed which offers more flexibility\n",
    "- If you need a proper database with all that databases offer you might prefer something like Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "years = range(1880, 2011)\n",
    "\n",
    "pieces = []\n",
    "columns = ['name', 'sex', 'births']\n",
    "\n",
    "for year in years:\n",
    "    path = datadir + 'yob%d.txt' % year\n",
    "    frame = dd.read_csv(path, names=columns)\n",
    "\n",
    "    frame['year'] = year\n",
    "    pieces.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Dask provides a [concatenate](http://docs.dask.org/en/latest/array-stack.html) function as well. It automatically ignores the indexes of objects to be concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate everything into a single Dask DataFrame \n",
    "names = dd.concat(pieces)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "names = names.categorize(\"sex\") # for dask dataframes, columns must be categorical, for pandas dataframes it is automatically inferred\n",
    "\n",
    "total_births = names.pivot_table(values='births', index='year', columns='sex', aggfunc='sum').compute() # Note: We are now converting to a pandas DataFrame using the compute() method.\n",
    "total_births.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_births.plot(title='Total births by sex and year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverting back to pandas DataFrame\n",
    "\n",
    "It is easy to start using pandas API at any point from a Dask DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = names.compute()\n",
    "type(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.groupby(['year', 'sex']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s insert a column `prop` with the fraction of babies given each name relative to the total number of births. A prop value of `0.02` would indicate that 2 out of every 100 babies were given a particular name. Thus, we group the data by year and sex, then add the new column to each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def add_prop(group):\n",
    "    group['prop'] = group.births / group.births.sum()\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = names.groupby(['year', 'sex']).apply(add_prop)\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing a group operation like this, it’s often valuable to do a sanity check, like verifying that the `prop` column sums to `1` within all the groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.groupby(['year', 'sex']).prop.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is done, I’m going to extract a subset of the data to facilitate further analysis: the top 1,000 names for each sex/year combination. This is yet another group operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_top1000(group):\n",
    "    return group.sort_values(by='births', ascending=False)[:1000]\n",
    "\n",
    "grouped = names.groupby(['year', 'sex'])\n",
    "\n",
    "top1000 = grouped.apply(get_top1000)\n",
    "\n",
    "# Drop the group index, not needed\n",
    "top1000.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer a do-it-yourself approach, try this instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pieces = []\n",
    "for year, group in names.groupby(['year', 'sex']):\n",
    "    pieces.append(group.sort_values(by='births', ascending=False)[:1000])\n",
    "top1000 = pd.concat(pieces, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataset is now quite a bit smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "top1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing Naming Trends\n",
    "\n",
    "With the full dataset and Top 1,000 dataset in hand, we can start analyzing various naming trends of interest. Splitting the Top 1,000 names into the boy and girl portions is easy to do first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "boys = top1000[top1000.sex == 'M']\n",
    "girls = top1000[top1000.sex == 'F']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple time series, like the number of Johns or Marys for each year, can be plotted but require a bit of munging to be more useful. Let’s form a pivot table of the total number of births by year and name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "total_births = top1000.pivot_table(values='births', index='year', columns='name', aggfunc=sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "total_births.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_births.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = total_births[['Tom', 'Dick', 'Harry', 'John', 'Mary', 'Marilyn']]\n",
    "\n",
    "subset.plot(subplots=True, figsize=(20, 20), grid=False, title=\"Number of births per year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On looking at this, you might conclude that these names have grown out of favor with the American population. But the story is actually more complicated than that, as will be explored in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Measuring the increase in naming diversity\n",
    "\n",
    "One explanation for the decrease in plots is that fewer parents are choosing common names for their children. This hypothesis can be explored and confirmed in the data. One measure is the proportion of births represented by the top 1,000 most popular names, which I aggregate and plot by year and sex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "table = top1000.pivot_table(values='prop', index='year', columns='sex', aggfunc=sum)\n",
    "\n",
    "table.plot(title='Sum of table1000.prop by year and sex',\n",
    "           yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10),\n",
    "           figsize=(14, 7), grid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, indeed, there appears to be increasing name diversity (decreasing total proportion in the top 1,000). Another interesting metric is the number of distinct names, taken in order of popularity from highest to lowest, in the top 50% of births. This number is a bit more tricky to compute. Let’s consider just the boy names from 2010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = boys[boys.year == 2010]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting prop in descending order, we want to know how many of the most popular names it takes to reach 50%. You could write a `for` loop to do this, but a vectorized NumPy way is a bit more clever. Taking the cumulative sum, `cumsum`, of prop and then calling the method `searchsorted` returns the position in the cumulative sum at which 0.5 would need to be inserted to keep it in sorted order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prop_cumsum = df.sort_values(by='prop', ascending=False).prop.cumsum()\n",
    "\n",
    "prop_cumsum[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since arrays are zero-indexed, adding 1 to this result gives you a result of 117. By contrast, in 1900 this number was much smaller as shown later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_cumsum.values.searchsorted(0.5) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = boys[boys.year == 1900]\n",
    "\n",
    "in1900 = df.sort_values(by='prop', ascending=False).prop.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1900.values.searchsorted(0.5) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now apply this operation to each year/sex combination, groupby those fields, and apply a function returning the count for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_quantile_count(group, q=0.5):\n",
    "    group = group.sort_values(by='prop', ascending=False)\n",
    "    return group.prop.cumsum().values.searchsorted(q) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)\n",
    "diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon unstacking, this resulting DataFrame diversity now has two time series, one for each sex, indexed by year. This can be inspected and plotted as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity = diversity.unstack('sex')\n",
    "diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "diversity.plot(title=\"Number of popular names in top 50%\", figsize=(14, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, girl names have always been more diverse than boy names, and they have only become more so over time. Further analysis of what exactly is driving the diversity, like the increase of alternative spellings, is left to the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### The “last letter” revolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2007, baby name researcher Laura Wattenberg pointed out on her [website](http://www.babynamewizard.com/) that the distribution of boy names by final letter has changed significantly over the last 100 years. To see this, we first aggregate all of the births in the full dataset by year, sex, and final letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# extract last letter from name column\n",
    "get_last_letter = lambda x: x[-1]\n",
    "\n",
    "last_letters = names.name.map(get_last_letter)\n",
    "last_letters.name = 'last_letter' # naming the series\n",
    "\n",
    "last_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sidenote: `apply` vs `applymap` vs `map`.\n",
    "\n",
    "- `DataFrame.apply` operates on entire rows or columns at a time.\n",
    "- `DataFrame.applymap`, `Series.apply`, and `Series.map` operate on one element at time.\n",
    "\n",
    "There is a lot of overlap between the capabilities of `Series.apply` and `Series.map`, meaning that either one will work in most cases. They do have some slight differences though, some of which are discussed in this [post](https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = names.pivot_table(values='births', index=last_letters, columns=['sex', 'year'], aggfunc='sum')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we select out three representative years spanning the history and print the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "subtable = table.reindex(columns=[1910, 1960, 2010], level='year')\n",
    "subtable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, normalize the table by total births to compute a new table containing proportion of total births for each sex ending in each letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "subtable.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_prop = subtable / subtable.sum()\n",
    "letter_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 16))\n",
    "letter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title=\"Proportion of boy names ending in each letter\")\n",
    "letter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title=\"Proportion of girl names ending in each letter\" , legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, boy names ending in `n` have experienced significant growth since the 1960s. Going back to the full table created before, we can again normalize by year and sex and select a subset of letters for the boy names, finally transposing to make each col‐ umn a time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots_adjust(hspace=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "letter_prop = table / table.sum()\n",
    "dny_ts = letter_prop.loc[['d', 'n', 'y'], 'M'].T\n",
    "dny_ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this DataFrame of time series in hand, we can make a plot of the trends over time again with its plot method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig = plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dny_ts.plot(figsize=(16, 10), title=\"Proportion of boys born with names ending in d/n/y over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Boy names that became girl names (and vice versa)\n",
    "\n",
    "Another fun trend is looking at boy names that were more popular with one sex earlier in the sample but have “changed sexes” in the present. One example is the name Lesley or Leslie. Going back to the top1000 DataFrame, We can compute a list of names occurring in the dataset starting with \"lesl\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_names = pd.Series(top1000.name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesley_like = all_names[all_names.str.lower().str.contains('lesl')]\n",
    "lesley_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we can filter down to just those names and sum births grouped by name to see the relative frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "filtered = top1000[top1000.name.isin(lesley_like)]\n",
    "\n",
    "filtered.groupby('name').births.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s aggregate by sex and year and normalize within year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "table = filtered.pivot_table(values='births', index='year', columns='sex', aggfunc='sum')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.div(table.sum(1), axis=0) # DataFrame.div() performs floating division of dataframe; DataFrame.sum(1) returns the sum of the values for axis 1, i.e., columns.\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, it’s now possible to make a plot of the breakdown by sex over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "table.plot(figsize=(16, 10), title = \"Proportion of male/female Lesley-like names over time\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
